import os
import json
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from multiprocessing.dummy import Pool as ThreadPool

# ========== åŸºç¡€é…ç½® ==========
BASE_URL = "https://www.xftoon.com"
DIRECTORY_URL = f"{BASE_URL}/comic/7264"
SAVE_ROOT = r"E:\å¨±ä¹\æ¼«ç”»"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Mobile Safari/537.36 Edg/115.0.1901.157',
    'Referer': DIRECTORY_URL
}

# ========== å·¥å…·å‡½æ•° ==========

def create_folder(folder_path):
    """åˆ›å»ºæ–‡ä»¶å¤¹"""
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"âœ… æ–‡ä»¶å¤¹å·²åˆ›å»º: {folder_path}")
    else:
        print(f"ğŸ“ æ–‡ä»¶å¤¹å·²å­˜åœ¨: {folder_path}")

def write_file(file_path, data):
    """å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(data)
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {file_path}")

def read_file(file_path):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

def fetch_html(url):
    """å¸¦é‡è¯•æœºåˆ¶çš„ç½‘é¡µè¯·æ±‚"""
    for i in range(3):
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ ({i+1}/3): {url} - {e}")
            time.sleep(2)
    print(f"âŒ å½»åº•å¤±è´¥: {url}")
    with open("error.txt", "a", encoding="utf-8") as err:
        err.write(f"{url}\n")
    return None

# ========== æ•°æ®è§£æéƒ¨åˆ† ==========

def fetch_directory_page():
    """ä¸‹è½½ç›®å½•é¡µå¹¶ä¿å­˜ä¸º HTML"""
    html = fetch_html(DIRECTORY_URL)
    if html:
        write_file("directory.html", html)

def parse_directory():
    """ä»ç›®å½•é¡µæå–ç« èŠ‚ JSON æ•°æ®"""
    html = read_file("directory.html")
    soup = BeautifulSoup(html, 'html.parser')
    data_script = soup.find_all('script')[11]
    json_text = str(data_script)[132:-519]  # xftoon ç‰¹å®šåç§»
    write_file("directory.json", json_text)
    print("ğŸ“‘ ç›®å½• JSON è§£æå®Œæˆ")

def get_title():
    """è·å–æ¼«ç”»æ ‡é¢˜+ä½œè€…"""
    soup = BeautifulSoup(read_file("directory.html"), 'html.parser')
    book_title = soup.select('.subHeader .BarTit')[0].get_text(strip=True)
    author_name = soup.select('.txtItme a')[0].get_text(strip=True)
    return f"{book_title} ({author_name})"

# ========== å›¾ç‰‡ä¸‹è½½éƒ¨åˆ† ==========

def download_image(image_url, path_name):
    """ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    if os.path.exists(path_name):
        print(f"âœ… å·²å­˜åœ¨: {path_name}")
        return
    for attempt in range(3):
        try:
            res = requests.get(image_url, headers=HEADERS, timeout=10)
            res.raise_for_status()
            with open(path_name, "wb") as f:
                f.write(res.content)
            print(f"ğŸ–¼ï¸ å·²ä¸‹è½½: {path_name}")
            return
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥ ({attempt+1}/3): {image_url} - {e}")
            time.sleep(2)
    with open("error.txt", "a", encoding="utf-8") as err:
        err.write(f"{image_url}\n")

def download_images(image_list, file_name, head_word):
    """å¤šçº¿ç¨‹ä¸‹è½½ç« èŠ‚å›¾ç‰‡"""
    create_folder(head_word)
    pool = ThreadPool(5)
    for img in image_list:
        img_url = img["imgUrl"]
        img_id = img["id"]
        suffix = os.path.splitext(os.path.basename(urlparse(img_url).path))[1]
        path_name = os.path.join(head_word, f"{img_id}_{file_name}{suffix}")
        pool.apply_async(download_image, (img_url, path_name))
    pool.close()
    pool.join()

# ========== é¡µé¢çˆ¬å–éƒ¨åˆ† ==========

def get_page_links(start_url, chapter_list):
    """é€’å½’è·å–é¡µé¢é“¾æ¥"""
    retries = 3
    for _ in range(retries):
        try:
            print(f"ğŸ“– æ­£åœ¨è·å–ç« èŠ‚åˆ†é¡µé“¾æ¥: {start_url}")
            soup = BeautifulSoup(requests.get(start_url, headers=HEADERS, timeout=10).text, 'html.parser')
            next_link_tag = soup.select('.letchepter .ChapterLestMune')
            if not next_link_tag:
                return chapter_list

            next_link = next_link_tag[0].get('href')
            next_text = next_link_tag[0].get_text(strip=True)

            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸‹ä¸€é¡µ
            if "ä¸‹ä¸€é¡µ" in next_text:
                # å¤„ç†æ­£å¸¸å’ŒçŸ­è·¯å¾„ä¸¤ç§æƒ…å†µ
                if len(next_link) > 12:
                    next_url = BASE_URL + next_link
                else:
                    script_data = soup.find_all('script')[12]
                    part1 = str(script_data)[50:54]
                    part2 = str(script_data)[79:85]
                    next_url = f"{BASE_URL}/view/{part1}/{part2}/{next_link}"

                if next_url not in chapter_list:
                    chapter_list.append(next_url)
                    return get_page_links(next_url, chapter_list)
            return chapter_list
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è·å–é¡µé¢é“¾æ¥é”™è¯¯: {start_url} - {e}")
            time.sleep(2)
            if _ == retries - 1:
                return chapter_list

def process_chapters():
    """è§£æç« èŠ‚å¹¶ä¸‹è½½æ‰€æœ‰å›¾ç‰‡"""
    json_array = json.loads(read_file("directory.json"))
    title_folder = os.path.join(SAVE_ROOT, get_title())
    create_folder(title_folder)

    for item in json_array:
        chapter_id = item["id"]
        chapter_title = item["title"].replace("?", "").replace(":", "").replace(".", "")
        start_url = f"{BASE_URL}/view/{item['comic_id']}/{chapter_id}"
        head_word = os.path.join(title_folder, chapter_title)

        print(f"\nğŸ“˜ å¼€å§‹å¤„ç†ç« èŠ‚: {chapter_title}")
        chapter_pages = get_page_links(start_url, [start_url])
        print(f"â¡ï¸ å…± {len(chapter_pages)} é¡µ")

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡é“¾æ¥
        image_list = []
        for page_url in chapter_pages:
            html = fetch_html(page_url)
            if not html:
                continue
            soup = BeautifulSoup(html, 'html.parser')
            imgs = soup.select('#commicBox .charpetBox img')
            for idx, img in enumerate(imgs, start=len(image_list) + 1):
                img_url = img.get('data-original', '').replace("httpss", "https")
                if img_url:
                    image_list.append({"id": idx, "imgUrl": img_url})

        # ä¸‹è½½ç« èŠ‚å›¾ç‰‡
        if image_list:
            print(f"ğŸ“¸ å…± {len(image_list)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            download_images(image_list, chapter_title, head_word)
        else:
            print(f"âš ï¸ æœªå‘ç°å›¾ç‰‡: {chapter_title}")

# ========== ä¸»ç¨‹åºå…¥å£ ==========

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨xftoon æ¼«ç”»ä¸‹è½½å™¨")
    open("error.txt", "w", encoding="utf-8").close()
    fetch_directory_page()
    parse_directory()
    process_chapters()
    print("ğŸ‰ æ‰€æœ‰ç« èŠ‚ä¸‹è½½å®Œæˆï¼")
